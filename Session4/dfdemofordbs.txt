from pyspark.sql import functions as f

spark.conf.set("spark.sql.shuffle.partitions",1)

cust_df = spark.read \
        .format("csv") \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .load("/FileStore/tables/custdata/custorders*.txt")
    

cust_grp_df = cust_df.groupBy("custno") \
                    .agg(f.sum("orderamt").alias("totalbiz"),f.count("orderno").alias("#orders"))
cust_grp_df.show()

cust_grp_df.write \
        .format("csv") \
        .mode("overwrite") \
        .option("path", "/FileStore/tables/custdata/grpdata/") \
        .save()